\section{Constructing Kernels}

\subsection{part a}

Give function $c(\cdot)$ represents conjunctions containing up to $k$ different literals, what it means is that $c(x_1)$ is checking if all literals in $c(\cdot)$ are active literals in example $x_1$. If yes, it has output of positive 1, otherwise it outputs 0. The same for $c(x_2)$. Then we know, $c(x_1)c(x_2)$ checks if all literals in $c(\cdot)$ are active literals both in example $x_1$ and $x_2$. If yes, outputs positive 1 otherwise 0.

The kernel $K(x_1, x_2)$ as stated in the problem is summing over all possible conjunctions $c(\cdot)$ evaluated on $x_1$ and $x_2$. If to compute the kernel function follows the function defined in the problem, the computational time required is not linear in $n$. So we need to propose a new approach to compute the kernel value.

We propose the following approach:

\begin{equation}
K(x_1,x_2) = \sum_{j=0}^{min(k, \text{numCommon($x_1$, $x_2$)}) } {\text{numCommon($x_1$, $x_2$)} \choose j}
\end{equation}

in which $\text{numCommon($x_1$, $x_2$)}$ returns the number of common literals shared by $x_1$ and $x_2$.

The idea of this approach is to first calculate the number of common literals shared by $x_1$ and $x_2$, and then formulate the class $C$ by choosing 1, 2 to up to $k$ literals from the common literals. Computing the total number of such $c$ gives us the exactly the same kernel value as defined in the problem statement. And the above equation helps us to compute this total number by summing over all possible $j's$. The number $j$ is up-bounded by the min(k, \text{numCommon($x_1$, $x_2$)}) to take care of the case where the number of common literals is bigger than $k$.

Now, we prove that our proposed approach gives the same result as the function defined in the problem statement. Observe the kernel function defined in the problem, the only conjunctions $c$ that will contribute to the kernel value are those returning value 1 given $x_1$ and $x_2$. And these conjunctions are exactly the ones we selected using the proposed approach since each $c$ from $C$ is guaranteed to return 1 evaluating on either $x_1$ or $x_2$. So it is sufficient only to consider these $c$.

Next, we prove the computational complexity of our proposed approach is linear in $n$. Given any two n-dimensional example $x_1$ and $x_2$, we are able to compute the number of their common literals in $\mathcal{O}(n)$. Then we compute $1!, 2!, ..., \text{numCommon($x_1$, $x_2$)}!$ in $\mathcal{O}(n)$. The final step is to perform summation of all chooses which could be finished in $\mathcal{O}(1)$. Therefore, the proposed approach could be computed in $\mathcal{O}(n)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{part b}
\begin{algorithm}[H]
To train the classifier:
 \KwData{Training examples denoted by $x_t$ with their corresponding label $y_t$.}
 \KwResult{Mistake counter $\vec{\alpha}$ with $m$ dimension.}
  set $\vec{\alpha}=0$;

 \For{every training example $(x_t,y_t)$}{
    $y_{pred} = sign(\sum_{i}^{m} \alpha_i y_i K(x_i, x_t))$

    \If{$y_{pred} \neq y_t$}{ update $\alpha_t$: $\alpha_t = \alpha_t+1$\;}
 }


return: $\alpha$\;

 \caption{Kernel Perceptron Algorithm}
\end{algorithm}

where $m$ denotes the number of training examples available, $\alpha_i$ denotes the number of mistakes made on example $i$.

After obtaining the classifier, in order to predict label for future examples, such as $x_{new}$, we follow:

\begin{equation}
y_{pred} = sign(\sum_{i}^{m} \alpha_i y_i K(x_i, x_{new}))
\end{equation}

where $x_i, y_i$ are provided by training data while $\alpha_i$ is returned by the training algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{part c}

One key point to notice is that running kernel Perceptron in the original sample space is the same as running original (or unmodified) Perceptron in the blown-up space. In our case, the blown-up space has $m$ dimensions. $m$ is the number of training samples available. Each training example $x$ is mapped to the new space $P$ according to the kernel function $K(x_i,x)$ where $i \in 1,2,...m$. To be more clear, training example $x_i$ is mapped to point $p_i = <K(x_1,x_i), K(x_2,x_i), ..., K(x_m, x_i)>$.

First, we try to find $R$. According to the Novikoff's theorem, for the original Perceptron, $R \geq \|x_i\|$ for any $i$. Inspired by this, in order to find the $R$ for the new space we need to find the maximum $p_i$ in the space $P$. Based on the kernel function specified in part a, we see that for any pair of examples, $K(\cdot,\cdot) \leq \sum_{j=0}^{n}{n \choose j}$ since the maximum number of common literals between any pair of examples is $n$. Therefore, we claim that $\|p_i\| \leq \sqrt{m ( \sum_{j=0}^{n}{n \choose j} ) ^2}$. We define $R = \sqrt{m ( \sum_{j=0}^{n}{n \choose j} ) ^2}$

Next, we try to find $\gamma$. Given by the problem statement, we know for sure we could learn a classifier that is consistent with all the training data. Therefore, in the new space $P$, we could compute the minimum distance between any two points with one point has positive label and the other point has negative label. The distance could be described as follows:

\begin{equation}
d = min \|p_1 - p_2\|
\end{equation}
where $p_1$ is any point from $P$ that has positive label while $p_2$ has negative label.

With the returned vector $\vec{\alpha}$, we know it satisfies the equation

\begin{equation}
y_i \alpha p_i\geq \frac{1}{2}d
\end{equation}

Normalizing $\alpha$ by dividing both side with $\|\alpha\|$ we have $y_i \alpha p_i \frac{1}{\|\alpha\|} \geq \frac{1}{2}d\frac{1}{\|\alpha\|}$.

Suppose each training example is being trained $k$ times, which means $\|\alpha\| \leq \sqrt{km}$ in which $km$ describes the worst possible case that all examples were misclassified $k$ times. Now, we substitute $\|\alpha\|$ back in the right hand side of the above equation and obtain

\begin{equation}
y_i \alpha p_i \frac{1}{\|\alpha\|} \geq \frac{1}{2}d\frac{1}{\sqrt{km}}
\end{equation}

in which $\alpha \frac{1}{\|\alpha\|}$ can be seen as the $u$ in the Novikoff's theorem. Therefore, we define

\begin{equation}
\gamma = \frac{1}{2}d\frac{1}{\sqrt{km}}
\end{equation}

At last, we conclude the maximum number of mistakes the kernel Perceptron is bounded by $\frac{R^2}{\gamma^2}$ in which $R$ and $\gamma$ are defined as above.







